{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install transformers\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®š APIKEY\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Intorduction to ğŸ¤— Hugging Face and Transformers Library**\n",
    "\n",
    "[ğŸ¤— hugging face NLP course](https://huggingface.co/learn/nlp-course/chapter1/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NLP æ˜¯èªè¨€å­¸å’Œæ©Ÿå™¨å­¸ç¿’é ˜åŸŸï¼Œå°ˆæ³¨æ–¼ç†è§£èˆ‡äººé¡èªè¨€ç›¸é—œçš„ä¸€åˆ‡ã€‚ NLP ä»»å‹™çš„ç›®æ¨™ä¸åƒ…æ˜¯å–®ç¨ç†è§£å–®å­—ï¼Œè€Œä¸”èƒ½å¤ ç†è§£é€™äº›å–®å­—çš„ä¸Šä¸‹æ–‡ã€‚</p>\n",
    "NLP å¸¸è¦‹çš„ä»»å‹™æœ‰ï¼š\n",
    "   1. **Classifying whole sentences æ–‡æœ¬åˆ†é¡ï¼ˆåˆ†é¡æ•´å¥ï¼‰**ï¼šå°‡æ•´å€‹å¥å­é€²è¡Œåˆ†é¡\n",
    "      - æƒ…æ„Ÿåˆ†æï¼šã€Œé€™éƒ¨é›»å½±å¾ˆæ£’ï¼ã€ -> positive\n",
    "      - åƒåœ¾éƒµä»¶æª¢æ¸¬ï¼šã€Œä½ çš„ iphone å·²è¢«åš´é‡æå£ã€-> spam \n",
    "   2. **Classifying each word in a sentence å–®è©åˆ†é¡**ï¼šå°ä¸€å¥è©±ä¸­çš„æ‰€æœ‰å­—é€²è¡Œåˆ†é¡\n",
    "      - èªæ³•åˆ†æï¼šã€Œä»–è·‘å¾—å¿«ã€-> ä»–ï¼ˆä»£è©ï¼‰è·‘ï¼ˆå‹•è©ï¼‰å¾—ï¼ˆå‰¯è©ï¼‰å¿«ï¼ˆå½¢å®¹è©ï¼‰\n",
    "      - å‘½åå¯¦é«” NER\n",
    "   3. **Sentence Generation**\n",
    "      1. **å¡«å……é®è”½è©**ï¼šã€Œåƒé€™ç¨®è¦æ±‚ï¼Œæˆ‘é€™è¼©å­[mask]ï¼ã€-> [mask] é æ¸¬ç‚º æ²’è½é\n",
    "      2. **è‡ªå‹•ç”Ÿæˆ**ï¼šã€Œä»Šå¤©å¤©æ°£å¦‚ä½•ã€-> ã€Œä»Šå¤©å¤©æ°£éå¸¸æ™´æœ—é©åˆå¤–å‡ºã€\n",
    "      3. **ç¿»è­¯**ï¼šã€Œä½ å¥½ã€-> ã€ŒHelloã€\n",
    "      4. **æ‘˜è¦ï¼ˆå•ç­”ï¼‰**\n",
    "         1. å¾æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ Extractive QAï¼šã€Œæ³•åœ‹çš„é¦–éƒ½æ˜¯å·´é»ã€‚   é¦–éƒ½åœ¨å“ªã€-> å·´é»ï¼ˆè—‰ç”±å·´é»åœ¨åŸæ–‡çš„ index æŠ“å‡ºä¾†ï¼‰\n",
    "         2. ä»¥ç”Ÿæˆæ¨¡å‹é€²è¡Œæ‘˜è¦ Generative QAï¼šã€Œæ³•åœ‹çš„é¦–éƒ½æ˜¯å·´é»ã€‚   é¦–éƒ½åœ¨å“ªã€-> é¦–éƒ½åœ¨å·´é»ï¼ˆä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚ chatgptï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`pipeline` in Hugging Face transformers**\n",
    "``transformers`` ç‚º ğŸ¤—Hugging face æä¾›çš„å¥—ä»¶ï¼Œè®“é–‹ç™¼è€…å¯ä»¥å‰µå»ºã€ä½¿ç”¨ Hugging face hub ä¸Š NLPã€LLM çš„æ¨¡å‹</p>\n",
    "> Hugging face hub ä¸Šçš„æ¨¡å‹ä¸åªæœ‰ transformerï¼Œä»»ä½•äººéƒ½å¯ä»¥ä¸Šå‚³ä»»ä½•é¡å‹çš„æ¨¡å‹æˆ–è³‡æ–™é›†\n",
    "\n",
    "åœ¨ `transformers` ä¸­æœ€é«˜éšçš„å‡½æ•¸æ˜¯ `pipeline`ï¼Œ\n",
    "è©²å‡½æ•¸å°‡ä½¿ç”¨æ¨¡å‹éœ€è¦çš„é è™•ç†ã€æ¨ç†èˆ‡å¾Œè™•ç†ä¸²é€£èµ·ä¾†ï¼Œ</p>\n",
    "å‚³å…¥æŒ‡å®šçš„ taskï¼Œ`pipeline` æœƒè‡ªå‹•ä»¥é©åˆçš„æ¨¡å‹é€²è¡Œæ¨ç†ï¼ˆé æ¸¬ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥å¾ [hub](https://huggingface.co/models) é€é Tasksã€Languages ç¯©é¸æ‰¾åˆ°è‡ªå·±æƒ³è¦æ‡‰ç”¨çš„æ¨¡å‹</p>\n",
    "å¾ 1. [task summary](https://huggingface.co/docs/transformers/task_summary) 2. [Tasks](https://huggingface.co/tasks) æ‰¾åˆ°æ”¯æ´çš„ NLP ç›¸é—œä»»å‹™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ä½¿ç”¨ pipeline å®Œæˆå¸¸è¦‹ NLP ä»»å‹™**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. æƒ…æ„Ÿåˆ†æ aka åˆ†é¡å•é¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998723268508911}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"sentiment-analysis\")\n",
    "pipe(\"this is awesome!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å‘½åå¯¦é«”</p>\n",
    "\n",
    "\n",
    "\n",
    "åœ¨ ner ä»»å‹™ä¸­ï¼Œæ¨¡å‹æœƒå°æ‰€æœ‰å­—è©ï¼ˆtokenï¼‰ é€²è¡Œåˆ†é¡ï¼Œå¾—åˆ°ï¼š</p>\n",
    "1. è©²å­—è©ï¼ˆtokenï¼‰å°æ‡‰çš„ entity\n",
    "2. score æ©Ÿç‡å€¼\n",
    "3. ä»¥åŠå°æ‡‰åˆ°æ–‡æœ¬çš„èµ·å§‹çµæŸä½ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(task=\"ner\", \n",
    "                # model='dslim/bert-base-NER'\n",
    "                )\n",
    "ner_pipe(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ç·´ç¿’ #1**\n",
    "\n",
    "ä½¿ç”¨ Extractive QA model ä»¥ `pipeline` åš question-answering ä»»å‹™ï¼š\n",
    "- **çµ¦å®šæ–‡æœ¬**ï¼šthe name of repo is bert-base-uncased\n",
    "- **å•é¡Œç›®æ¨™**ï¼šå•æ¨¡å‹ repo çš„åç¨±\n",
    "- **é æœŸç­”æ¡ˆ**ï¼šbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# practice 1 ä¸éœ€è¦ç‰¹åˆ¥æŒ‡å®šæ¨¡å‹ï¼Œpipeline é è¨­è¼‰å…¥ distilbert-base-cased-distilled-squad, \n",
    "# å…¶ç‚º Extractive QA é¡æ‘˜è¦æ¨¡å‹\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **åˆ©ç”¨ Conversation class èˆ‡ text-generation model å¯¦ä½œ chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import pipeline\n",
    "from transformers import BitsAndBytesConfig, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ç‚ºè¼‰å…¥æ¨¡å‹è¼ƒå¤§ï¼Œä½¿ç”¨ T4 GPU æ™‚å»ºè­°é€²è¡Œé‡åŒ–ï¼Œä»¥ä¸‹ç¨‹å¼ç‚ºé‡åŒ–è™•ç†éç¨‹ï¼Œ</p>\n",
    "åœ¨æ­¤å…ˆä¸è´…è¿°ï¼Œæœ‰èˆˆè¶£çš„å¯ä»¥åƒè€ƒ Hugging face å®˜æ–¹æ–‡ä»¶ï½</p>\n",
    "\n",
    "èˆ‡å‰é¢ç¯„ä¾‹ä¸åŒçš„æ˜¯ï¼Œæ¨¡å‹è¼‰å…¥æ–¹æ³•ï¼Œæˆ‘å€‘é€é `AuToModelForCausalLM` å¯¦ä¾‹åŒ–æ¨¡å‹ï¼Œå°‡å…¶ä½œç‚ºåƒæ•¸å‚³å…¥ `pipeline`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'MediaTek-Research/Breeze-7B-32k-Instruct-v1_0'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id)\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=hf_model, \n",
    "    tokenizer=tokenizer, # Tokenizerï¼Œè¦èˆ‡æ¨¡å‹åŒ¹é…ï¼Œä¸»è¦æä¾› chat æ¨¡å¼æ™‚çš„ç‰¹æ®Šç¬¦è™Ÿ\n",
    "    max_new_tokens=1024, # æ¨¡å‹æœ€å¤šå¯ä»¥ç”Ÿæˆå¤šå°‘å­—\n",
    "    return_full_text=False # æ§åˆ¶ pipeline åªè¼¸å‡º AI Message\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "èŠå¤©å¼æ¨¡å‹ï¼Œä¾‹å¦‚ ChatGPT å…¶å¯¦åŸºæœ¬ä¸Šæ˜¯é€é text-generation ä½œç‚ºåŸºç¤æ¨¡å‹ï¼Œé€²ä¸€æ­¥è¨“ç·´æ¨¡å‹èƒ½éèŠå¤©ã€‚\n",
    "æ‰€ä»¥æ¨¡å‹çš„é¸æ“‡ï¼Œæˆ‘å€‘å¯ä»¥åœ¨ Huggingface ä¸Šæ‰¾åˆ° text-generation ä»»å‹™çš„æ¨¡å‹ï¼Œæ‡‰è©²éƒ½å¯ä»¥æ”¯æ´ã€‚</p>\n",
    "\n",
    "æ¯”è¼ƒç‰¹åˆ¥çš„æ˜¯ï¼Œè¦åšèŠå¤©ä»»å‹™æ™‚ï¼Œæ¨¡å‹éœ€è¦ä¸€äº›ç‰¹æ®Šç¬¦è™Ÿä¾†å€åˆ¥æ¯ä¸€æ®µè¨Šæ¯æ˜¯ä¾†è‡ªæ–¼ User æˆ–æ˜¯ AI é‚„æ˜¯ System Prompt</p>\n",
    "è€Œå„å€‹æ¨¡å‹çš„ç‰¹æ®Šç¬¦è™Ÿä¸ç›¡ç›¸åŒï¼Œéœ€è¦å»æŸ¥é–±å®˜æ–¹æ–‡ä»¶ã€‚ä¾‹å¦‚ Demo ä½¿ç”¨çš„è¯ç™¼ç§‘ Breeze æ¨¡å‹æ˜¯é€é `[INST]` ã€ `[/INST]` ä»¥åŠ `<s>` ä½œç‚ºå€éš”ã€‚\n",
    "æ‰€ä»¥æˆ‘å€‘åœ¨ä½¿ç”¨æ¨¡å‹æ™‚ï¼Œå°±æœƒéœ€è¦å°‡æ–‡å­—åŠ ä¸Šé€™äº›ç‰¹æ®Šç¬¦è™Ÿæ‰èƒ½å¤ ç™¼æ®æ¨¡å‹èŠå¤©çš„èƒ½åŠ›ã€‚</p>\n",
    "\n",
    "é€šå¸¸æˆ‘å€‘æœƒä½¿ç”¨ list of dict çš„æ–¹å¼è™•å­˜èŠå¤©çš„è¨˜éŒ„ï¼Œä½¿ç”¨ role å€åˆ¥ user èˆ‡ aiï¼Œcontent ä»£è¡¨å…§å®¹ï¼Œè€Œ Hugging face çš„æ¨¡å‹ä¹Ÿæ”¯æ´é€™æ¨£çš„æ ¼å¼ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"å—¨ä½ å¥½å—\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"å—¨æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„ AI åŠ©ç†ï¼Œå¾ˆé«˜èˆˆç‚ºæ‚¨æœå‹™ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"æ°æ°\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"æ°æ°ï¼ŒæœŸå¾…å†ç›¸è¦‹\"},\n",
    "\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "æˆ‘å€‘å¯ä»¥é€éå¯¦ä¾‹åŒ– Conversation é€™å€‹ classï¼Œé€é `add_user_input` èˆ‡ `append_response` æ–°å¢æ­·å²ä½¿ç”¨è€…è¼¸å…¥èˆ‡æ¨¡å‹å›è¦†ï¼Œå°‡è³‡æ–™è®Šç‚ºä¸Šè¿°çš„è³‡æ–™æ ¼å¼å†é€çµ¦æ¨¡å‹é€²è¡Œæ¨ç†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation\n",
    "conversation = Conversation() # å»ºç«‹ä¸€å€‹å°è©± Conversation ç‰©ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ©ç”¨ `add_user_input` æ–°å¢ user èŠå¤©è¨˜éŒ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç›®å‰èŠå¤©è¨˜éŒ„ï¼š[{'role': 'user', 'content': 'provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?'}]\n"
     ]
    }
   ],
   "source": [
    "conversation.add_user_input(\"provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?\")\n",
    "print(f\"ç›®å‰èŠå¤©è¨˜éŒ„ï¼š{conversation.messages}\") # conversation.messages å¯ä»¥ç›´æ¥ä¸Ÿçµ¦ chatbot å¾—åˆ°å›è¦†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'æ ¹æ“šæä¾›çš„ä¿¡æ¯ï¼Œrepoçš„åç§°æ˜¯\"bert-base-uncased\"ã€‚'}]\n"
     ]
    }
   ],
   "source": [
    "# å°‡ conversation.messages ä¸Ÿçµ¦ chatbot\n",
    "chatbot_result = chatbot(conversation.messages)\n",
    "print(chatbot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°‡ chatbot çš„å›è¦†ä»¥ `append_respons` çš„æ–¹æ³•åŠ å…¥ conversation ä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç›®å‰èŠå¤©è¨˜éŒ„ï¼š[{'role': 'user', 'content': 'provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?'}, {'role': 'assistant', 'content': 'æ ¹æ“šæä¾›çš„ä¿¡æ¯ï¼Œrepoçš„åç§°æ˜¯\"bert-base-uncased\"ã€‚'}]\n"
     ]
    }
   ],
   "source": [
    "conversation.append_response(chatbot_result[0]['generated_text'])\n",
    "print(f\"ç›®å‰èŠå¤©è¨˜éŒ„ï¼š{conversation.messages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç›®å‰èŠå¤©è¨˜éŒ„ï¼š[{'role': 'user', 'content': 'provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?'}, {'role': 'assistant', 'content': 'æ ¹æ“šæä¾›çš„ä¿¡æ¯ï¼Œrepoçš„åç§°æ˜¯\"bert-base-uncased\"ã€‚'}, {'role': 'user', 'content': 'é‚£ä»€éº¼æ˜¯ bert?'}]\n"
     ]
    }
   ],
   "source": [
    "conversation.add_user_input(\"é‚£ä»€éº¼æ˜¯ bert?\")\n",
    "\n",
    "print(f\"ç›®å‰èŠå¤©è¨˜éŒ„ï¼š{conversation.messages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM å›è¦†ï¼š[{'generated_text': ' Bertæ˜¯Bidirectional Embedding Representations from Transformersçš„ç¼©å†™ï¼Œæ˜¯Googleçš„ä¸€ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚å®ƒåŸºäºTransformersæ¶æ„ï¼Œå¯ä»¥å¯¹æ–‡æœ¬æ•°æ®å­¦ä¹ è¯­ä¹‰è¡¨è¾¾ï¼Œç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€æƒ…å†µè¯­è¨€æ¨¡å‹ç­‰ã€‚Bertæ¨¡å‹é€šè¿‡é¢„è®­ç»ƒåœ¨å¤§é‡çš„éä»»åŠ¡ä¸“ç”¨æ•°æ®ä¸Šè·å¾—çŸ¥è¯†ï¼Œç„¶ååœ¨ç‰¹å®šçš„ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚'}]\n",
      "----------\n",
      "ç›®å‰èŠå¤©è¨˜éŒ„ï¼š[{'role': 'user', 'content': 'provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?'}, {'role': 'assistant', 'content': 'æ ¹æ“šæä¾›çš„ä¿¡æ¯ï¼Œrepoçš„åç§°æ˜¯\"bert-base-uncased\"ã€‚'}, {'role': 'user', 'content': 'é‚£ä»€éº¼æ˜¯ bert?'}, {'role': 'assistant', 'content': ' Bertæ˜¯Bidirectional Embedding Representations from Transformersçš„ç¼©å†™ï¼Œæ˜¯Googleçš„ä¸€ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚å®ƒåŸºäºTransformersæ¶æ„ï¼Œå¯ä»¥å¯¹æ–‡æœ¬æ•°æ®å­¦ä¹ è¯­ä¹‰è¡¨è¾¾ï¼Œç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€æƒ…å†µè¯­è¨€æ¨¡å‹ç­‰ã€‚Bertæ¨¡å‹é€šè¿‡é¢„è®­ç»ƒåœ¨å¤§é‡çš„éä»»åŠ¡ä¸“ç”¨æ•°æ®ä¸Šè·å¾—çŸ¥è¯†ï¼Œç„¶ååœ¨ç‰¹å®šçš„ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚'}]\n"
     ]
    }
   ],
   "source": [
    "chatbot_result = chatbot(conversation.messages)\n",
    "print(f\"LLM å›è¦†ï¼š{chatbot_result}\")\n",
    "\n",
    "print(\"-\"*10)\n",
    "conversation.append_response(chatbot_result[0]['generated_text'])\n",
    "print(f\"ç›®å‰èŠå¤©è¨˜éŒ„ï¼š{conversation.messages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **embedding model (feature extraction)**\n",
    "[åƒè€ƒ](https://huggingface.co/tasks/feature-extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding æ˜¯å°‡æ–‡å­—è½‰æ›æˆå‘é‡çš„æŠ€è¡“ï¼Œä½¿å¾—æ–‡å­—å¯ä»¥åœ¨æ•¸å­¸ç©ºé–“ä¸­è¡¨ç¤ºã€‚</p>é€™äº›å‘é‡æ•æ‰äº†æ–‡å­—ä¹‹é–“çš„èªç¾©é—œä¿‚ï¼Œä½¿å¾—ç›¸ä¼¼çš„æ–‡å­—åœ¨å‘é‡ç©ºé–“ä¸­æ›´æ¥è¿‘ã€‚å¸¸è¦‹çš„åµŒå…¥æ¨¡å‹åŒ…æ‹¬ Word2Vecã€GloVe å’Œ BERT ç­‰ã€‚\n",
    "\n",
    "åœ¨ Retrieval-Augmented Generation (RAG) ä¸­ï¼Œæˆ‘å€‘æœƒç”¨ Embedding model ç”¨ä¾†å°‡æŸ¥è©¢ï¼ˆqueryï¼‰å’Œå€™é¸æ–‡æª”ï¼ˆdocumentï¼‰è½‰æ›æˆå‘é‡ã€‚</p>\n",
    "é€šéè¨ˆç®—é€™äº›å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œå¯ä»¥æ‰¾å‡ºèˆ‡æŸ¥è©¢æœ€ç›¸é—œçš„æ–‡æª”ã€‚é€™äº›ç›¸é—œæ–‡æª”éš¨å¾Œç”¨ä¾†ç”Ÿæˆå›ç­”ï¼Œå¢å¼·ç”Ÿæˆæ¨¡å‹çš„æº–ç¢ºæ€§å’Œä¸Šä¸‹æ–‡ç›¸é—œæ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘å€‘ä½¿ç”¨ sentence transformers é€™å€‹å¥—ä»¶ï¼Œå¯ä»¥å¾ [å®˜æ–¹æ–‡æª”](https://sbert.net/docs/pretrained_models.html) å°‹æ‰¾è‡ªå·±å¸Œæœ›ä½¿ç”¨çš„ modelï¼Œä¹Ÿå¯ä»¥åœ¨ hugging face å¹³å°ä¸Šæœå°‹æ”¯æ´ feature-extraction çš„æ¨¡å‹ã€‚</p>\n",
    "\n",
    "å¦å¤– hugging face ä¹Ÿæä¾› [embedding model](https://huggingface.co/spaces/mteb/leaderboard) çš„æ’è¡Œæ¦œçµ¦å¤§å®¶åƒè€ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/verdict/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03142083, -0.01894771, -0.00766944, ..., -0.02039895,\n",
       "       -0.01210877,  0.03742645], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åˆ©ç”¨ encode å¾—åˆ° sentence çš„ embedding\n",
    "embedding_model.encode(\"å“ˆå›‰ï¼Œé€™æ˜¯ä¸€å€‹å¥å­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ç‚ºä»€éº¼ ML éœ€è¦åšæ­£è¦åŒ–\"\n",
    "\n",
    "source_sentence = [\n",
    "    'Regularization is important!',\n",
    "    'Dropout is important!',\n",
    "    'Missing Data Handling is important!'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç•¶æˆ‘å€‘æœ‰æ¯å€‹å¥å­çš„ embedding å¾Œå°±å¯ä»¥é€é cosine similarity è¨ˆç®—æ¯å€‹æ–‡æœ¬çš„ç›¸ä¼¼åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç‚ºä»€éº¼ ML éœ€è¦åšæ­£è¦åŒ– vs Regularization is important! similarity: 0.8665976524353027\n",
      "ç‚ºä»€éº¼ ML éœ€è¦åšæ­£è¦åŒ– vs Dropout is important! similarity: 0.8055095672607422\n",
      "ç‚ºä»€éº¼ ML éœ€è¦åšæ­£è¦åŒ– vs Missing Data Handling is important! similarity: 0.8144524693489075\n",
      "==========\n",
      "èˆ‡ã€Œç‚ºä»€éº¼ ML éœ€è¦åšæ­£è¦åŒ–ã€æœ€ç›¸ä¼¼æ–‡æœ¬ï¼šRegularization is important!\n"
     ]
    }
   ],
   "source": [
    "most_related_sentence = None\n",
    "max_similarity = 0\n",
    "\n",
    "for sentence in source_sentence:\n",
    "    sim = calculate_cosine_similarity(\n",
    "    embedding_model.encode(query),\n",
    "    embedding_model.encode(sentence)\n",
    "    )\n",
    "    \n",
    "    if sim > max_similarity:\n",
    "        most_related_sentence = sentence\n",
    "        max_similarity = sim\n",
    "        \n",
    "    print(f\"{query} vs {sentence} similarity: {sim}\")\n",
    "    \n",
    "print(\"=\"*10)\n",
    "print(f\"èˆ‡ã€Œ{query}ã€æœ€ç›¸ä¼¼æ–‡æœ¬ï¼š{most_related_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ä½œæ¥­ demo**\n",
    "\n",
    "åˆ©ç”¨ Hugging Face çš„ text-generation model èˆ‡ Sentence Transformers embedding model å¯¦ä½œ QA æª¢ç´¢èŠå¤©æ©Ÿå™¨äººã€‚</p>\n",
    "åŸºæ–¼æä¾›çš„è³‡æ–™é›†ï¼Œä½¿ç”¨ Embedding Cosine Similarity æª¢ç´¢åƒè€ƒè³‡æ–™ï¼Œå†é€é LLM ç”Ÿæˆç­”æ¡ˆã€‚</p>\n",
    "\n",
    "1. Baseline\n",
    "   - å°‡ demo ä¸­çš„è³‡æ–™ï¼Œæ›¿æ›æˆæˆ‘å€‘æä¾› or è‡ªå·±çš„è³‡æ–™é›†\n",
    "   - èƒ½å¤ æª¢ç´¢ç›¸ä¼¼è³‡æ–™\n",
    "   - åŸºæ–¼æª¢ç´¢çš„è³‡æ–™é€²è¡Œå›ç­”\n",
    "2. Advancedï¼ˆOptionalï¼‰\n",
    "   - Embedding æ€éº¼å„²å­˜ï¼Ÿæ¯æ¬¡éƒ½è¦é‡æ–°è¨ˆç®—å—ï¼Ÿ\n",
    "   - è©²å¦‚ä½•è™•ç†å¤ªä¹…ä»¥å‰çš„æ­·å²è³‡æ–™ï¼Ÿ\n",
    "   - åˆ©ç”¨ Gradio or Hugging Face Spaces éƒ¨ç½²ã€åˆ†äº« Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = [\n",
    "    \"\"\"1. å…¥ç‡Ÿæ³¨æ„äº‹é …ï¼šå½¹ç”·å…¥ç‡Ÿå‰ç„¡é ˆå…ˆè¡Œç†é«®ï¼Œå¾…å…¥ç‡Ÿå¾Œçµ±ä¸€å®‰æ’ç†é«®ï¼Œå†å‘å½¹ç”·æ”¶å–è²»ç”¨\n",
    "    2. å½¹ç”·å¯æ”œå¸¶æ‰‹æ©Ÿ(ä¸å¾—ç‚ºå¤§é™¸å» ç‰Œï¼Œä¸”ä¸æä¾›å……é›»)ï¼Œéƒ¨éšŠæœƒé›†ä¸­ä¿ç®¡ï¼Œæ–¼æ¯æ—¥å¤œé–“ä»¥å®šæ™‚å®šé»æ–¹å¼ä½¿ç”¨ã€‚\n",
    "    3. ç‡Ÿå€ä¸æä¾›å……é›»ï¼Œå»ºè­°æ”œå¸¶æ‹‹æ£„å¼(æ‰‹å‹•)åˆ®é¬åˆ€ï¼›ä¸å¯æ”œå¸¶å™´éœ§å¼æ¶²é«”ã€‚\n",
    "    4. å½¹ç”·å¯é…æˆ´éš±å½¢çœ¼é¡ï¼Œä½†å› åŸºç¤è¨“ç·´æœŸé–“ç”Ÿæ´»ç·Šæ¹Šï¼Œè€Œéš±å½¢çœ¼é¡éœ€ç›¸ç•¶æ™‚é–“æ¶ˆæ¯’æ¸…æ´—ï¼Œå»ºè­°é…æˆ´é¡ç‰‡çœ¼é¡ç‚ºå®œï¼Œä¸¦å¯å¤šå‚™ä¸€å‰¯çœ¼é¡ï¼Œä»¥ä¾›æ›¿æ›ã€‚\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"å ±åˆ°å…¥ç‡Ÿæ™‚æ‡‰è©²æ–‡ä»¶ï¼š\n",
    "    1. å¾µé›†ä»¤ã€‚\n",
    "    2. å½¹ç”·æœ¬äººä¹‹åœ‹æ°‘èº«åˆ†è­‰æ­£æœ¬ã€‚\n",
    "    3. ç§ç« ã€‚\n",
    "    4. å¥ä¿ï¼©ï¼£å¡ã€‚\n",
    "    5. éƒµå±€å­˜æ‘ºæ­£é¢å½±æœ¬ã€‚\n",
    "    6. æ›¿ä»£å½¹å½¹ç”·è¼”å°éœ€æ±‚èª¿æŸ¥è¡¨ã€‚\n",
    "    7. å€‹äººç‰¹æ®Šé†«ç™‚ç”¨å“ã€‚\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    æ›¿ä»£å½¹è¨“ç·´å¤©æ•¸\n",
    "    åŸºç¤è¨“ç·´ï¼š21å¤©(å«æ’¥äº¤æ—¥)ã€‚(è‡ª253æ¢¯æ¬¡èµ·ä¿®æ­£)\n",
    "    å°ˆæ¥­è¨“ç·´ï¼šä»¥å„åˆ†ç™¼éœ€ç”¨æ©Ÿé—œä¸åŒè€Œæœ‰å€‹ä¸åŒæœŸé–“çš„å°ˆæ¥­è¨“ç·´ã€‚\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    æŠ˜æŠµå½¹æœŸè¦å®š\n",
    "    è«‹å‚™å¦¥ç›¸é—œè­‰æ˜æ–‡ä»¶ï¼Œå¦‚é«˜ä¸­ï¼ˆè·ï¼‰ä»¥ä¸Šå„ç´šå­¸æ ¡ç¶“è»è¨“ä¸»ç®¡é©—è­‰åŠ è¨»æŠ˜æŠµå½¹æœŸæ—¥æ•¸ä¹‹æˆç¸¾å–®ã€å¤§å°ˆé›†è¨“çµè¨“è­‰æ›¸ã€\n",
    "    é©—é€€ï¼ˆåœå½¹ï¼‰è­‰æ˜æ›¸æˆ–è»äº‹å­¸æ ¡é€€å­¸ï¼ˆé–‹é™¤ï¼‰è­‰æ˜æ›¸ç­‰æ­£æœ¬ï¼ˆé©—è­‰å¾Œé€€é‚„ï¼‰ã€‚82å¹´æ¬¡ä»¥å‰å‡ºç”Ÿè€…ï¼Œ\n",
    "    åˆè¨ˆä¸å¾—é€¾30æ—¥ï¼Œ83-93å¹´æ¬¡ä»¥å¾Œå‡ºç”Ÿè€…ï¼Œåˆè¨ˆä¸å¾—é€¾15æ—¥ã€‚å·²å—è»äº‹å…¥ä¼è¨“ç·´è€…ï¼Œè«‹æ–¼æ”¶åˆ°å¾µé›†ä»¤æ™‚ï¼Œå‘æˆ¶ç±åœ°å€å…¬æ‰€æå‡ºå…å—åŸºç¤è¨“ç·´ç”³è«‹ã€‚\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    è¦ç•¶å…µæ™‚ï¼Œå¥ä¿è¦è¾¦ç†è½‰å‡ºè½‰å…¥å—?\n",
    "    å³å°‡å…¥ç‡Ÿæœå¸¸å‚™å…µå½¹ä¹‹å½¹ç”·éœ€æŒå¾µé›†ä»¤å‘å…¨æ°‘å¥ä¿åŠ ä¿å–®ä½è¾¦ç†è½‰å‡ºï¼Œè½‰å‡ºæ—¥æœŸå¡«å ±ã€Œå…¥ç‡Ÿç•¶æœˆä»½ã€ï¼Œè½‰å…¥å–®ä½å…å¡«ï¼Œå¾ŒçºŒå°‡ç”±åœ‹è»å–®ä½æ¥çºŒè¾¦ç†å¾ŒçºŒè½‰å…¥ç¨‹åºã€‚\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"ç•¶å…µæœŸé–“ç¾©å‹™å½¹è–ªæ°´çš„å…¥å¸³æˆ¶ä¸€å®šè¦éƒµå±€å—?\n",
    "    ä¾ç¾è¡Œè¦å®šç›®å‰æœ‰éƒµå±€ã€å°æ–°ã€åœŸåœ°éŠ€è¡Œè·Ÿåˆä½œé‡‘åº«é€™å››å®¶é‡‘èæ©Ÿæ§‹å¯ä½¿ç”¨ã€‚\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def get_answer(query: str, source: List[str]):\n",
    "    most_related_sentence = None\n",
    "    max_similarity = 0\n",
    "\n",
    "    for sentence in source:\n",
    "        sim = calculate_cosine_similarity(\n",
    "        embedding_model.encode(query),\n",
    "        embedding_model.encode(sentence)\n",
    "        )\n",
    "        \n",
    "        if sim > max_similarity:\n",
    "            most_related_sentence = sentence\n",
    "            max_similarity = sim\n",
    "            \n",
    "    return most_related_sentence\n",
    "\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: æˆ‘æ˜å¤©è¦å»è»ç‡Ÿå ±åˆ°ï¼Œæˆ‘æ‡‰è©²è¦å¸¶ä»€éº¼æ–‡ä»¶\n",
      "AI: æ˜å¤©ä½ è¦å»è»ç‡Ÿå ±åˆ°ï¼Œä½ æ‡‰è©²è¦å¸¶ä»¥ä¸‹æ–‡ä»¶ï¼š\n",
      "\n",
      "1. å¾µé›†ä»¤\n",
      "2. å½¹ç”·æœ¬äººä¹‹åœ‹æ°‘èº«åˆ†è­‰æ­£æœ¬\n",
      "3. ç§ç« \n",
      "4. å¥ä¿ICå¡\n",
      "5. éƒµå±€å­˜æ‘ºæ­£é¢å½±æœ¬\n",
      "6. æ›¿ä»£å½¹å½¹ç”·è¼”å°éœ€æ±‚èª¿æŸ¥è¡¨\n",
      "7. å€‹äººç‰¹æ®Šé†«ç™‚ç”¨å“\n",
      "\n",
      "ä»¥ä¸Šæ–‡ä»¶è«‹å‹™å¿…å‚™å¦¥ä¸¦ç¢ºèªå¾Œï¼Œæ˜å¤©å‰å¾€è»ç‡Ÿé€²è¡Œå ±åˆ°ã€‚\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\">>>\")\n",
    "conversation = Conversation()\n",
    "\n",
    "while user_query.lower() != \"bye\":\n",
    "    print(f\"user: {user_query}\")\n",
    "    # å°‹æ‰¾æœ€ç›¸ä¼¼çš„æ–‡ä»¶\n",
    "    answer = get_answer(user_query, qa_data)\n",
    "    llm_input = f\"\"\"è«‹ä½ åŸºæ–¼ä»¥ä¸‹è³‡è¨Šå›ç­”ä½¿ç”¨è€…çš„å•é¡Œ\n",
    "    {answer}\n",
    "    ===\n",
    "    å•é¡Œï¼š{user_query}\n",
    "    \"\"\"\n",
    "    conversation.add_user_input(llm_input)\n",
    "    # å°‡ conversation.messages ä¸Ÿçµ¦ chatbot\n",
    "    chatbot_result = chatbot(conversation.messages)[0]['generated_text']\n",
    "    print(f\"AI: {chatbot_result}\")\n",
    "    conversation.append_response(chatbot_result)\n",
    "    \n",
    "    user_query = input(\">>>\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
