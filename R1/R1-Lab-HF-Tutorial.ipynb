{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install transformers\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®š APIKEY\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Intorduction to ğŸ¤— Hugging Face and Transformers Library**\n",
    "\n",
    "[ğŸ¤— hugging face NLP course](https://huggingface.co/learn/nlp-course/chapter1/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NLP æ˜¯èªè¨€å­¸å’Œæ©Ÿå™¨å­¸ç¿’é ˜åŸŸï¼Œå°ˆæ³¨æ–¼ç†è§£èˆ‡äººé¡èªè¨€ç›¸é—œçš„ä¸€åˆ‡ã€‚ NLP ä»»å‹™çš„ç›®æ¨™ä¸åƒ…æ˜¯å–®ç¨ç†è§£å–®å­—ï¼Œè€Œä¸”èƒ½å¤ ç†è§£é€™äº›å–®å­—çš„ä¸Šä¸‹æ–‡ã€‚</p>\n",
    "NLP å¸¸è¦‹çš„ä»»å‹™æœ‰ï¼š\n",
    "   1. **Classifying whole sentences æ–‡æœ¬åˆ†é¡ï¼ˆåˆ†é¡æ•´å¥ï¼‰**ï¼šå°‡æ•´å€‹å¥å­é€²è¡Œåˆ†é¡\n",
    "      - æƒ…æ„Ÿåˆ†æï¼šã€Œé€™éƒ¨é›»å½±å¾ˆæ£’ï¼ã€ -> positive\n",
    "      - åƒåœ¾éƒµä»¶æª¢æ¸¬ï¼šã€Œä½ çš„ iphone å·²è¢«åš´é‡æå£ã€-> spam \n",
    "   2. **Classifying each word in a sentence å–®è©åˆ†é¡**ï¼šå°ä¸€å¥è©±ä¸­çš„æ‰€æœ‰å­—é€²è¡Œåˆ†é¡\n",
    "      - èªæ³•åˆ†æï¼šã€Œä»–è·‘å¾—å¿«ã€-> ä»–ï¼ˆä»£è©ï¼‰è·‘ï¼ˆå‹•è©ï¼‰å¾—ï¼ˆå‰¯è©ï¼‰å¿«ï¼ˆå½¢å®¹è©ï¼‰\n",
    "      - å‘½åå¯¦é«” NER\n",
    "   3. **Sentence Generation**\n",
    "      1. **å¡«å……é®è”½è©**ï¼šã€Œåƒé€™ç¨®è¦æ±‚ï¼Œæˆ‘é€™è¼©å­[mask]ï¼ã€-> [mask] é æ¸¬ç‚º æ²’è½é\n",
    "      2. **è‡ªå‹•ç”Ÿæˆ**ï¼šã€Œä»Šå¤©å¤©æ°£å¦‚ä½•ã€-> ã€Œä»Šå¤©å¤©æ°£éå¸¸æ™´æœ—é©åˆå¤–å‡ºã€\n",
    "      3. **ç¿»è­¯**ï¼šã€Œä½ å¥½ã€-> ã€ŒHelloã€\n",
    "      4. **æ‘˜è¦ï¼ˆå•ç­”ï¼‰**\n",
    "         1. å¾æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ Extractive QAï¼šã€Œæ³•åœ‹çš„é¦–éƒ½æ˜¯å·´é»ã€‚   é¦–éƒ½åœ¨å“ªã€-> å·´é»ï¼ˆè—‰ç”±å·´é»åœ¨åŸæ–‡çš„ index æŠ“å‡ºä¾†ï¼‰\n",
    "         2. ä»¥ç”Ÿæˆæ¨¡å‹é€²è¡Œæ‘˜è¦ Generative QAï¼šã€Œæ³•åœ‹çš„é¦–éƒ½æ˜¯å·´é»ã€‚   é¦–éƒ½åœ¨å“ªã€-> é¦–éƒ½åœ¨å·´é»ï¼ˆä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚ chatgptï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`pipeline` in Hugging Face transformers**\n",
    "``transformers`` ç‚º ğŸ¤—Hugging face æä¾›çš„å¥—ä»¶ï¼Œè®“é–‹ç™¼è€…å¯ä»¥å‰µå»ºã€ä½¿ç”¨ Hugging face hub ä¸Š NLPã€LLM çš„æ¨¡å‹</p>\n",
    "> Hugging face hub ä¸Šçš„æ¨¡å‹ä¸åªæœ‰ transformerï¼Œä»»ä½•äººéƒ½å¯ä»¥ä¸Šå‚³ä»»ä½•é¡å‹çš„æ¨¡å‹æˆ–è³‡æ–™é›†\n",
    "\n",
    "åœ¨ `transformers` ä¸­æœ€é«˜éšçš„å‡½æ•¸æ˜¯ `pipeline`ï¼Œ\n",
    "è©²å‡½æ•¸å°‡ä½¿ç”¨æ¨¡å‹éœ€è¦çš„é è™•ç†ã€æ¨ç†èˆ‡å¾Œè™•ç†ä¸²é€£èµ·ä¾†ï¼Œ</p>\n",
    "å‚³å…¥æŒ‡å®šçš„ taskï¼Œ`pipeline` æœƒè‡ªå‹•ä»¥é©åˆçš„æ¨¡å‹é€²è¡Œæ¨ç†ï¼ˆé æ¸¬ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥å¾ [hub](https://huggingface.co/models) é€é Tasksã€Languages ç¯©é¸æ‰¾åˆ°è‡ªå·±æƒ³è¦æ‡‰ç”¨çš„æ¨¡å‹</p>\n",
    "å¾ 1. [task summary](https://huggingface.co/docs/transformers/task_summary) 2. [Tasks](https://huggingface.co/tasks) æ‰¾åˆ°æ”¯æ´çš„ NLP ç›¸é—œä»»å‹™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ä½¿ç”¨ pipeline å®Œæˆå¸¸è¦‹ NLP ä»»å‹™**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. æƒ…æ„Ÿåˆ†æ aka åˆ†é¡å•é¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998723268508911}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"sentiment-analysis\")\n",
    "pipe(\"this is awesome!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å‘½åå¯¦é«”</p>\n",
    "\n",
    "\n",
    "\n",
    "åœ¨ ner ä»»å‹™ä¸­ï¼Œæ¨¡å‹æœƒå°æ‰€æœ‰å­—è©ï¼ˆtokenï¼‰ é€²è¡Œåˆ†é¡ï¼Œå¾—åˆ°ï¼š</p>\n",
    "1. è©²å­—è©ï¼ˆtokenï¼‰å°æ‡‰çš„ entity\n",
    "2. score æ©Ÿç‡å€¼\n",
    "3. ä»¥åŠå°æ‡‰åˆ°æ–‡æœ¬çš„èµ·å§‹çµæŸä½ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(task=\"ner\", \n",
    "                # model='dslim/bert-base-NER'\n",
    "                )\n",
    "ner_pipe(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ç·´ç¿’ #1**\n",
    "\n",
    "ä½¿ç”¨ Extractive QA model ä»¥ `pipeline` åš question-answering ä»»å‹™ï¼š\n",
    "- **çµ¦å®šæ–‡æœ¬**ï¼šthe name of repo is bert-base-uncased\n",
    "- **å•é¡Œç›®æ¨™**ï¼šå•æ¨¡å‹ repo çš„åç¨±\n",
    "- **é æœŸç­”æ¡ˆ**ï¼šbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# practice 1 ä¸éœ€è¦ç‰¹åˆ¥æŒ‡å®šæ¨¡å‹ï¼Œpipeline é è¨­è¼‰å…¥ distilbert-base-cased-distilled-squad, \n",
    "# å…¶ç‚º Extractive QA é¡æ‘˜è¦æ¨¡å‹\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **åˆ©ç”¨ Conversation class èˆ‡ text-generation model å¯¦ä½œ chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "from transformers import pipeline\n",
    "from transformers import BitsAndBytesConfig, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ç‚ºè¼‰å…¥æ¨¡å‹è¼ƒå¤§ï¼Œä½¿ç”¨ T4 GPU æ™‚å»ºè­°é€²è¡Œé‡åŒ–ï¼Œä»¥ä¸‹ç¨‹å¼ç‚ºé‡åŒ–è™•ç†éç¨‹ï¼Œ</p>\n",
    "åœ¨æ­¤å…ˆä¸è´…è¿°ï¼Œæœ‰èˆˆè¶£çš„åŒä»å¯ä»¥åƒè€ƒ Huggingface å®˜æ–¹æ–‡ä»¶ã€‚</p>\n",
    "\n",
    "èˆ‡å‰é¢ç¯„ä¾‹ä¸åŒçš„æ˜¯ï¼Œæ¨¡å‹è¼‰å…¥æ–¹æ³•ï¼Œæˆ‘å€‘é€é `AuToModelForCausalLM` å¯¦ä¾‹åŒ–æ¨¡å‹ï¼Œå°‡å…¶ä½œç‚ºåƒæ•¸å‚³å…¥ `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'MediaTek-Research/Breeze-7B-32k-Instruct-v1_0'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=hf_model, \n",
    "    tokenizer=tokenizer, # Tokenizerï¼Œè¦èˆ‡æ¨¡å‹åŒ¹é…ï¼Œä¸»è¦æä¾› chat æ¨¡å¼æ™‚çš„ç‰¹æ®Šç¬¦è™Ÿ\n",
    "    max_new_tokens=1024, # æ¨¡å‹æœ€å¤šå¯ä»¥ç”Ÿæˆå¤šå°‘å­—\n",
    "    return_full_text=False # æ§åˆ¶ pipeline åªè¼¸å‡º AI Message\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "èŠå¤©å¼æ¨¡å‹ï¼Œä¾‹å¦‚ ChatGPT å…¶å¯¦åŸºæœ¬ä¸Šæ˜¯é€é text-generation ä½œç‚ºåŸºç¤æ¨¡å‹ï¼Œé€²ä¸€æ­¥è¨“ç·´æ¨¡å‹èƒ½éèŠå¤©ã€‚\n",
    "æ‰€ä»¥æ¨¡å‹çš„é¸æ“‡ï¼Œæˆ‘å€‘å¯ä»¥åœ¨ Huggingface ä¸Šæ‰¾åˆ° text-generation ä»»å‹™çš„æ¨¡å‹ï¼Œæ‡‰è©²éƒ½å¯ä»¥æ”¯æ´ã€‚</p>\n",
    "\n",
    "æ¯”è¼ƒç‰¹åˆ¥çš„æ˜¯ï¼Œè¦åšèŠå¤©ä»»å‹™æ™‚ï¼Œæ¨¡å‹éœ€è¦ä¸€äº›ç‰¹æ®Šç¬¦è™Ÿä¾†å€åˆ¥æ¯ä¸€æ®µè¨Šæ¯æ˜¯ä¾†è‡ªæ–¼ User æˆ–æ˜¯ AI é‚„æ˜¯ System Prompt</p>\n",
    "è€Œå„å€‹æ¨¡å‹çš„ç‰¹æ®Šç¬¦è™Ÿä¸ç›¡ç›¸åŒï¼Œéœ€è¦å»æŸ¥é–±å®˜æ–¹æ–‡ä»¶ã€‚ä¾‹å¦‚ Demo ä½¿ç”¨çš„è¯ç™¼ç§‘ Breeze æ¨¡å‹æ˜¯é€é `[INST]` ã€ `[/INST]` ä»¥åŠ `<s>` ä½œç‚ºå€éš”ã€‚\n",
    "æ‰€ä»¥æˆ‘å€‘åœ¨ä½¿ç”¨æ¨¡å‹æ™‚ï¼Œå°±æœƒéœ€è¦å°‡æ–‡å­—åŠ ä¸Šé€™äº›ç‰¹æ®Šç¬¦è™Ÿæ‰èƒ½å¤ ç™¼æ®æ¨¡å‹èŠå¤©çš„èƒ½åŠ›ã€‚</p>\n",
    "\n",
    "é€šå¸¸æˆ‘å€‘æœƒä½¿ç”¨ list of dict çš„æ–¹å¼è™•å­˜èŠå¤©çš„è¨˜éŒ„ï¼Œä½¿ç”¨ role å€åˆ¥ user èˆ‡ aiï¼Œcontent ä»£è¡¨å…§å®¹ï¼Œè€Œ Hugging face çš„æ¨¡å‹ä¹Ÿæ”¯æ´é€™æ¨£çš„æ ¼å¼ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"å—¨ä½ å¥½å—\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"å—¨æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„ AI åŠ©ç†ï¼Œå¾ˆé«˜èˆˆç‚ºæ‚¨æœå‹™ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"æ°æ°\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"æ°æ°ï¼ŒæœŸå¾…å†ç›¸è¦‹\"},\n",
    "\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "æˆ‘å€‘å¯ä»¥é€éå¯¦ä¾‹åŒ– Conversation é€™å€‹ classï¼Œé€é `add_user_input` èˆ‡ `append_response` æ–°å¢æ­·å²ä½¿ç”¨è€…è¼¸å…¥èˆ‡æ¨¡å‹å›è¦†ï¼Œå°‡è³‡æ–™è®Šç‚ºä¸Šè¿°çš„è³‡æ–™æ ¼å¼å†é€çµ¦æ¨¡å‹é€²è¡Œæ¨ç†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation\n",
    "conversation = Conversation() # å»ºç«‹ä¸€å€‹å°è©± Conversation ç‰©ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.add_user_input(\"provided information: the name of repo is bert-base-uncased. Based on the provided information, what is the name of repo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_result = chatbot(conversation.messages)[-1]['generated_text']\n",
    "conversation.append_response(chatbot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.add_user_input(\"é‚£ä»€éº¼æ˜¯ bert?\")\n",
    "chatbot_result = chatbot(conversation.messages)[-1]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Bertæ˜¯Bidirectional Embedding Representations from Transformersçš„ç¼©å†™ï¼Œæ˜¯Googleçš„ä¸€ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚å®ƒåŸºäºTransformersæ¶æ„ï¼Œå¯ä»¥å¯¹æ–‡æœ¬æ•°æ®å­¦ä¹ è¯­ä¹‰è¡¨è¾¾ï¼Œç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€æƒ…å†µè¯­è¨€æ¨¡å‹ç­‰ã€‚Bertæ¨¡å‹é€šè¿‡é¢„è®­ç»ƒåœ¨å¤§é‡çš„éä»»åŠ¡ä¸“ç”¨æ•°æ®ä¸Šè·å¾—çŸ¥è¯†ï¼Œç„¶ååœ¨ç‰¹å®šçš„ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Practi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **embedding model (feature extraction)**\n",
    "[åƒè€ƒ](https://huggingface.co/tasks/feature-extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"feature-extraction\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ç‚ºä»€éº¼ ML éœ€è¦åšæ­£è¦åŒ–\"\n",
    "\n",
    "source_sentence = [\n",
    "    'Regularization is important!',\n",
    "    'Dropout is important!',\n",
    "    'Missing Data Handling is important!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç‚ºä»€éº¼ ML éœ€è¦åšæ­£è¦åŒ– vs Regularization is important! similarity: 0.8665976524353027\n",
      "ç‚ºä»€éº¼ ML éœ€è¦åšæ­£è¦åŒ– vs Dropout is important! similarity: 0.8055095672607422\n",
      "ç‚ºä»€éº¼ ML éœ€è¦åšæ­£è¦åŒ– vs Missing Data Handling is important! similarity: 0.8144524693489075\n",
      "èˆ‡ Query æœ€ç›¸é—œæ–‡æœ¬ï¼šRegularization is important!\n"
     ]
    }
   ],
   "source": [
    "most_related_sentence = None\n",
    "max_similarity = 0\n",
    "\n",
    "for sentence in source_sentence:\n",
    "    sim = calculate_cosine_similarity(\n",
    "    embedding_model.encode(query),\n",
    "    embedding_model.encode(sentence)\n",
    "    )\n",
    "    \n",
    "    if sim > max_similarity:\n",
    "        most_related_sentence = sentence\n",
    "        max_similarity = sim\n",
    "        \n",
    "    print(f\"{query} vs {sentence} similarity: {sim}\")\n",
    "    \n",
    "print(f\"èˆ‡ Query æœ€ç›¸é—œæ–‡æœ¬ï¼š{most_related_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
